# -*- coding: utf-8 -*-
"""cse351_hw2_Shuhood_Guhfran_114483164.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ogT6FfSSk4wMEyV2FmqC8UL9Iq22_1Sw
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import warnings # I HAVE THIS TO SUPPRESS WARNINGS ON MISSING VALUES (not errors)
warnings.simplefilter(action = "ignore", category = RuntimeWarning)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df = pd.read_csv('/kaggle/input/new-york-city-airbnb-open-data/AB_NYC_2019.csv')
df.head()

"""# Question 1

There are several columns - id, name, host_id, host_name, neightbourhood_group, and so on with almost 49000 entries.
"""

# mask = df.isna() # Figure out which entries are missing via True or not (False)
mask = df.isna().sum() # Figure out number of missing values in column, 16 missing in name, 21 in host_name, and 10052 for last_review & reviews_permonth
print(mask)

"""After a close inspection of dataframe using df.head() and df.tail(), I found the two columns 'name' and 'host_name' to be not that important in general for the assignment. This is because there is only 16 and 21 missing values, respectively, from ~ 49k records. The significance of having accurate imputation is minimal in affecting the dataset.

As a result, I will impute the name field with a value of the form: "room_type" in "neighbourhood" columns to indicate relevant values. Additionally, as for host_name, I will impute "Anonymous".
"""

mask = df['name'].isna() # Get which records are empty (true)
df.loc[mask, 'name'] = df['room_type'].fillna('') + ' ' + df['neighbourhood'].fillna('') # Fill empty value of the format: "Room_type in neighbourhood_name"
df['host_name'] = df['host_name'].fillna('Unknown') # As for hostname, impute "Unknown"
print(df.isna().sum()) # Check status of missing values now

"""Now that the two columns have been taken care of, now I need to focus on last_review & reviews_per_month. Both of these have way too many values missing, almost 20% of the dataset for these columns respectively. An interesting point is that both are missing the exact same amount, which could be the rows are related. After further inspection of the data, I found they are indeed related to one another."""

out = df[(df['last_review'].isna()) | (df['reviews_per_month'].isna())] # Return all rows where either last_review or reviews_per_month is missing
# out.describe()
# out.tail()
out.head()

"""As it can be seen, the columns are indeed related as the average, min, and max for the number_of_reviews column is 0 and there are precisely 10,052 records indicating the relation of missing both values when this is the case. If there are 0 reviews for these, then I can impute 0 for "reviews_per_month" column, and a custom value like "Unreviewed" for "last_review" column."""

df['reviews_per_month'] = df['reviews_per_month'].fillna(0) # Fill missing values with "0" for reviews_per_month column
df['last_review'] = df['last_review'].fillna('Unreviewed') # Fill missing values of last_review with "Unreviewed"
print(df.isna().sum())

df.describe()

"""Even though missing values were fixed with imputation, there is a problem as can be seen above with outliers. For example, the max value for minimum_nights reads 1250 days, which is almost 4 years! And, there is a max of 10,000 dollars for daily price. This is unreasonable for a per-day basis as mean for price column is around 152 dollars, a reasonable average daily price for airbnbs. And, there is a problem where there is airbnb with price 0. For that, I will get a count to see how many have price 0"""

out = df[df['price'] == 0] # Get only listings with price = 0
out.describe()

"""Given that there are only 11 records with price = 0, I will remove these 11 records as the count is insignificant for the larger dataset."""

# Remove price = 0 columns, 11 only
df = df[df["price"] > 0] # Filter out such listings
df.describe() # Only 11 removed, 48895 --> 48884 (count)

out = df[df["price"] < 20] # Current minimum airbnb listing in New York is $ 20 (checked online on 03/28/2025)
out.head(n=10)

"""Furthermore, there are 0 listings below 10 dollars. The minimum is 10 dollars and that is fine because I can easily find 20 dollar listings currently, so this can be considered valid listings adjust for "reverse" inflation into the past. Now, I will focus on the outliers on other side. Likewise, I will repeat the case for the following columns: minimum_nights, number_of_reviews, reviews_per_month, and calculated_host_listings_count. I will use the 75% value to see how bad the outliers are."""

# Only plot histograms for these columns
selected_columns = ['price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count']

# Plot histograms
fig, axes = plt.subplots(1, len(selected_columns), figsize=(12, 5))

for ax, col in zip(axes, selected_columns):
    df[col].plot(kind='hist', ax=ax, color='skyblue', edgecolor='black')
    ax.set_xlabel(col)
    ax.grid(True)

plt.tight_layout() # minimize spacing between plots
plt.show()

"""As we can see, price above 5000 dollars is extremely unreasonable. But, I found most of these to be luxury apartments. Since price = 0 does not make sense, but price being 5000+ dollars could be reasonable for luxury apartments in an insanely expensive area. Additinally, these are few values, over a dataset of ~49k, they will have close to no influence on actual mean. Likewise, anything above 500 for minimum_nights, 500 for number_of_reviews, and 50 for reviews_per_month is suspicious, but they do not negatively affect the data as values like 0 minimum_nights would. The calculated_host_listings_count is fine as there are enough listings around that price. Now, I will move onto question 2.

# Question 2

To find out the top 5 and bottom 5 neighbourhoods, I need to first group by neighbourhood and filter out the neighbourhoods with at most 5 listings. Then, I have to figure out average price of listings in neighbourhood. I group by neighbourhood_groups also to get average price of a neighbourhood as there may or may not be two neighbourhoods with exact same name in different neighbourhood groups. After, I sort the neighbourhoods by average price and via neighbourhood groups to finally be able to extract the top 5 and bottom 5 listings in each neighbourhood group as seen below.
"""

out = df[['neighbourhood_group', 'neighbourhood', 'price']] # we don't need the rest of columns
out = out.groupby('neighbourhood').filter(lambda x: len(x) > 5) # we only need neighbourhoods with more than 5 listings
# average prices by neighbourhood
out = out.groupby(['neighbourhood_group', 'neighbourhood'])['price'].mean().reset_index() # We want to average for neighbourhoods and their groups (there can be 2 neighbourhoods with same name from different groups)
out.rename(columns={'price': 'average_price'}, inplace=True) # For readability
out = out.sort_values(by=["neighbourhood_group", "average_price"]) # sort so that we have it groups by neighbourhood_group with each neighbourhood
print(out)

"""**Bottom 5 Listings for Each Neighbourhood Group:**"""

# Bottom 5 Neighbourhoods based on average price for each neighbourhood group
bottom_5 = out.groupby('neighbourhood_group').head(n=5) # It's in ascending order
print(bottom_5)

"""**Top 5 Listings for Each Neighbourhood Group:**"""

# TOP 5 Neighbourhoods based on average price for each neighbourhood group
top_5 = out.groupby('neighbourhood_group').tail(n=5)
print(top_5)

"""Now, to determine the extent of price variation, I will use the mean from previous example to plot a Histogram for each neighbourhood group. I use 500 as the limit for y-axis since the mean is around 100 for all neighbourhood groups, but so many outliers in price and beyond the upper quartile range. I think the limit 500 shows a good understanding of how these 'outliers' vary by each neighbourhood group too."""

out = df[['neighbourhood_group', 'price']] # We don't need the other columns
# out = out.groupby('neighbourhood_group')['price'].mean().reset_index() # Group by neighbourhood group, calculate the average.

plt.figure(figsize=(10, 6))
sns.boxplot(data=out, x='neighbourhood_group', y='price')

plt.title("Price Distribution by Neighbourhood Group")
plt.xlabel("Neighbourhood Group")
plt.ylabel("Price")
plt.ylim(0, 500) # limit upper end to avoid bad output
plt.show()

"""# Question 3

**Set of interesting features: price, minimum_nights, number_of_reviews, calculated_host_listings**

I will plot the following:
1. price vs minimum_nights correlation
2. price vs number_of_reviews correlation
3.
"""

columns = ['price', 'minimum_nights']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Price & Minimum Nights Correlation")
plt.show()

columns = ['price', 'number_of_reviews']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Price & Number of Reviews Correlation")
plt.show()

columns = ['price', 'calculated_host_listings_count']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Price & Calculated Host Listings Correlation")
plt.show()

columns = ['minimum_nights', 'number_of_reviews']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Minimum Nights & Number of Reviews Correlation")
plt.show()

"""**Strongest positive correlation 0.06 between price & calculated host listings**

**Strongest anti correlation -0.08 between minimum nights and the number of reviews**

# Question 4

I use seaborn with matplotlib to plot a scatterplot using longitude for x-axis, latitude for y-axis, and neighbourhood_group as hue for color coding. I stick to default "Set1" for the actual coloring, which is nice enough visually to differentiate the different boroughs of New York City.
"""

plt.figure(figsize=(10,6))
sns.scatterplot(data=df, x='longitude', y='latitude', hue='neighbourhood_group', palette='Set1')
plt.title('Scatter Plot of Airbnb Listings by Longitude & Latitude')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Color-Coded Neighbourhood Groups')
plt.show()

"""First, I filter for Airbnb listings that are at most 1000 dollars. Then, I make a copy to avoid the SettingWithCopyWarning. Then, I create bins for ranges 0-100, 100-250, 250-500, 500-1000 alongside labels to use with the scatter plot. Then, I create a price_range column which labels different listings with the labels within the ranges described for bins for easily plotting a scatter plot. Now, I use these unequal ranges because the motivation is that a rich person will not care if a listing is 800 dollars or 900 dollars, whereas the poor person will certainly care about a difference between 50 and 80 dollar listing. Additionally, this is just me trying to show the essence of wealth distribution - a power law based phenomena."""

out = df[df['price'] < 1000] # Filter for only listings below $ 1,000
out = out.copy() # Avoid SettingWithCopyWarning warning

bins = [0, 100, 250, 500, 1000]
labels = ['0-100', '100-250', '250-500', '500-1000']
out.loc[:, 'price_range'] = pd.cut(out['price'], bins=bins, labels=labels, right=False) # Create a price_range column using the bins & labels

plt.figure(figsize=(10,6))
sns.scatterplot(data=out, x='longitude', y='latitude', hue='price_range', palette='Blues') # Choose palette Blues for cleaner shading of which area is more expensive
plt.title('Scatter Plot of Airbnb Places by Longitude & Latitude')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Color-Coded Price Ranges')
plt.show()

"""# [](http://)Question 5
**I used wordcloud library as it was allowed by professor on piazza**
"""

text = " ".join(df['name'].dropna()) # Join all name values into one single string
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text) # create wordcloud
plt.figure(figsize=(10,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Airbnb Listings')
plt.show()

"""# Question 6

First, I create bins and labels just like previous scatter plots of color-coding by Host Listings Count value. Then, I create the column that labels using the provided bins and labels list. Then, I use orange palette to show which areas were more busy as ornage indicates "hot" in a heatmap type map - in this case the color-coded scatter plot acts a lot like a heatmap. As it can be seen, the majority of listings are in Manhattan or more accurately, the majority of high listing hosts are in Manhattan.
"""

out = df
bins = [0, 10, 25, 50, 100, 250, 10000]
labels = ['0-10', '10-25', '25-50', '50-100', '100-250', '250-10000']
out.loc[:, 'hostings_range'] = pd.cut(df['calculated_host_listings_count'], bins=bins, labels=labels, right=False) # Create a price_range column using the bins & labels

plt.figure(figsize=(10,6))
sns.scatterplot(data=out, x='longitude', y='latitude', hue='hostings_range', palette='Oranges') # Choose palette Oranges to stress busy areas
plt.title('Scatter Plot of Airbnb Listings by Longitude & Latitude')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Color-Coded Host Listings Count')
plt.show()

"""To put a number to each neighbourhood group, I did the average of calculated host listings and as it can be seen, Manhattan far surpasses in host listings than other boroughs."""

out = df[["neighbourhood_group", "calculated_host_listings_count"]]
out = out.groupby('neighbourhood_group')['calculated_host_listings_count'].mean().reset_index()
out.head() # There are only 5 neighbourhood groups aka boroughs

"""Now, I plot a bunch of correlations to further examine the claim and as it can be seen from the following correlations plus the ones from Question 3, the strongest correlation is between availability 365 and calculated host listings. This gives us a natural insight - many of Manhattan properties for Airbnb are readily available because they exist to make money from Airbnb. Additionally, another strong correlation was between Calculated Host Listings and Minimum Nights of r = 0.13. Other correlations were not as strong such as the one between price & calculated host listings correlation of around 0.06  """

columns = ['availability_365', 'calculated_host_listings_count']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Availability 365 & Calculated Host Listings Count Correlation")
plt.show()

columns = ['price', 'calculated_host_listings_count']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Price & Calculated Host Listings Count Correlation")
plt.show()

columns = ['number_of_reviews', 'calculated_host_listings_count']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Number of Reviews & Calculated Host Listings Correlation")
plt.show()

columns = ['minimum_nights', 'calculated_host_listings_count']
correlation_matrix = df[columns].corr(method='pearson')
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt="0.2f", cmap="coolwarm", center=0)
plt.title("Calculated Host Listings Count & Minimum Nights Correlation")
plt.show()

"""# Question 7

**Unique Plot 1 - Scatterplot of longitude/latitude, color-coding for number of reviews**

I consider this interesting because it could be considered in terms of finding locations that are heavily discussed, or perhaps have the most number of negative reviews driving up traffic, or even for algorithmic updates based on number of reviews of given locations.
"""

out = df # reset out variable to dataframe
bins = [0, 50, 150, 300, 600]
labels = ['0-50', '50-150', '150-300', '300-600']
out.loc[:, 'reviews_range'] = pd.cut(out['number_of_reviews'], bins=bins, labels=labels, right=False) # Create a price_range column using the bins & labels

plt.figure(figsize=(10,6))
sns.scatterplot(data=out, x='longitude', y='latitude', hue='reviews_range', palette='Greens') # Choose palette Oranges to stress busy areas
plt.title('Scatter Plot of Airbnb Listings by Longitude & Latitude')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Color-Coded Number of Reviews')
plt.show()

"""The below is an average on number of reviews to further corrobarate which areas were picked on the scatter plot."""

out = df # reset out variable to dataframe
out = out.groupby(['neighbourhood', 'neighbourhood_group'])['number_of_reviews'].mean().reset_index()
out.rename(columns={'number_of_reviews': 'average_number_of_reviews'}, inplace=True) # For readability
out = out.sort_values(by=["average_number_of_reviews"], ascending=False) # sort so that we have it groups by neighbourhood_group with each neighbourhood
out.head(n=20)

"""**Unique Plot 2 - Piechart of Each Neighbourhood Group's Share of Listings**

I consider this also an important chart because we need to know which neighbourhood groups dominate majority of the listings. As we can see, mainly two - Brooklyn and Manhattan dominate the Airbnb listings. This is important as it can be useful information on further understanding the nature of the dataset, and for making predictions based on some model.
"""

plt.figure(figsize=(10, 10))
out = df

neighbourhood_counts = out['neighbourhood_group'].value_counts() # Count up each neighbourhood group
wedges, texts, autotexts = plt.pie(neighbourhood_counts, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('muted', len(neighbourhood_counts)), wedgeprops={'edgecolor': 'black'}) # plot the chart
plt.legend(wedges, neighbourhood_counts.index, title="Neighbourhood Group", loc="center left", bbox_to_anchor=(1, 0.5)) # Add legends

plt.title('Distribution of Properties by Neighbourhood Group')
plt.show()